{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\175091\\AppData\\Local\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\175091\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-1.3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   40, 16726,   262,  2854,   286,   402, 11571,    12,  8199,    78,\n",
      "         4166,   416,  4946, 20185,    13])\n",
      "I evaluated the performance of GPT-Neo developed by OpenAI.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input = tokenizer.encode(\"I evaluated the performance of GPT-Neo developed by OpenAI.\", return_tensors='pt')\n",
    "\n",
    "print(input[0])\n",
    "\n",
    "print(tokenizer.decode(input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40, 16726,   262,  2854,   286,   402, 11571,    12,  8199,    78,\n",
      "          4166,   416,  4946, 20185,    13],\n",
      "        [   40, 16726,   262,  2854,   286,   402, 11571,  4166,   416,  4946,\n",
      "         20185,    13, 50257, 50257, 50257]])\n",
      "['I evaluated the performance of GPT-Neo developed by OpenAI.', 'I evaluated the performance of GPT developed by OpenAI.[PAD][PAD][PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "input = tokenizer.batch_encode_plus([\"I evaluated the performance of GPT-Neo developed by OpenAI.\",\"I evaluated the performance of GPT developed by OpenAI.\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(input['input_ids'])\n",
    "\n",
    "print([tokenizer.decode(input['input_ids'][i]) for i in range(len(input['input_ids']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40, 16726,   262,  2854,   286],\n",
       "        [   53,  4134,   500,   329,   649],\n",
       "        [   18,    13,  1415, 19707, 22980]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer.batch_encode_plus([\"I evaluated the performance of GPT2 developed by OpenAI.\", \"Vaccine for new coronavirus in the UK\", \"3.1415926535\"], max_length=5, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated = model.generate(input['input_ids'])\n",
    "\n",
    "len(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1\n",
      "I evaluated the performance of the proposed method on the real-world dataset. The results are shown in Table \\[tab:\n",
      "\n",
      "No.2\n",
      "Vaccine for new-borns\n",
      "\n",
      "The vaccine for new-borns is a vaccine for the prevention of diseases\n",
      "\n",
      "No.3\n",
      "3.141592653589793238462643383279502884197169399375105820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.batch_decode(generated)\n",
    "\n",
    "for i, sentence in enumerate(generated_text):\n",
    "    print(f'No.{i+1}')\n",
    "    print(f'{sentence}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
